training_config:
  experiment_name: 'pretrain_default'
  out_path: 'results/rnn_pretraining'
  num_workers: 1
  max_iters: 1000000
  batch_size: 256
  data_augmentation: False
  batch_end_callback_every: 10000
  val_molecules_to_sample: 1000
  early_stopping_patience: 20
  early_stopping_metric: 'val_loss'
  early_stopping_eps: 0.01
  early_stopping_should_go_down: True
  descriptor: 'smiles'
  keep_best_only: True
  balance_classes: False
hyperparameters:
  device: 'auto'
  weight_decay: 0.0001
  lr: 0.0003
  vocabulary_size: 36
  token_embedding_dim: 128
  seq_length: 102
  rnn_type: 'lstm'
  rnn_hidden_size: 512
  rnn_num_layers: 3
  rnn_dropout: 0
  grad_norm_clip: 5
